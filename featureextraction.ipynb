{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import lxml\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction from HLTV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook only contains the section to extract features from the HLTV player pages used later for building a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfiltered players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the player names from each year must be collected from HLTV's repo. Since HLTV did not publish advanced statistics until late 2015, only the years 2016, 2017, 2018, 2019 are used. \n",
    "\n",
    "Only LAN matches are counted, while no filter restriction is placed on the Player Filter. The **_oneYear(year)_** function is a helper function that returns a string that has the date range depending on the year supplied. Each year has its own HTML file, and the collection of all four HTML files are placed in the 'unfilteredplayers' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneYear(year):\n",
    "    return 'startDate=' + str(year) + '-01-01&endDate=' + str(year) + '-12-31'\n",
    "\n",
    "# for year in np.arange(2016,2020):\n",
    "#     url = 'https://www.hltv.org/stats/players?' + oneYear(year) + '&matchType=Lan'\n",
    "#     temp = requests.get(url).text\n",
    "#     with open('unfilteredplayers/players' + str(year) + '.html', 'w', encoding='utf-8') as f:\n",
    "#         f.write(temp)\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These html files now must be read and turned into .csv files. The **_allplayersonLAN(year)_** function below takes in a year and reads that year's HTML page that has the list of players who have official statistics on LAN events only. A dictionary is returned that maps players' names to players' individual statistic page based on the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allplayersonLAN(year):\n",
    "    with open('unfilteredplayers/players' + str(year) + '.html','r',encoding='utf-8') as players_html:\n",
    "        soup = BeautifulSoup(src,'html.parser')\n",
    "    temp = {}\n",
    "    for player in soup.find_all('td',class_='playerCol'):\n",
    "        temp[player.text] = 'https://hltv.org' + player.find('a')['href']\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **_htmlToCsvUnfiltered(year)_** function is the main function that transforms the HTML files into CSV files. It uses **_allplayersonLAN_** as a helper function to produce a dictionary, which is then converted to a DataFrame and then exported as a .csv. These .csv files are then placed in 'unfilteredplayers' as well, accompanying the .html files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def htmlToCsvUnfiltered(year): \n",
    "    playerlist = allplayersonLAN(year)\n",
    "    tab = pd.DataFrame.from_dict(playerlist,orient='index',columns=['Webpage'])\n",
    "    tab.to_csv('unfilteredplayers/players' + str(year) + '.csv')\n",
    "    print(str(year) + \"-Number of unfiltered players: \" + str(len(playerlist)))\n",
    "    return\n",
    "\n",
    "# for year in np.arange(2016,2020):\n",
    "#     htmlToCsvUnfiltered(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did a player make HLTV's Top 20?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section 2, **_aggregateTop20()_** produced a DataFrame of all the Top 20 lists for the four years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTop20(year, index):\n",
    "    url = 'https://www.hltv.org/news/' + str(index) + '/top-20-players-of-' + str(year) + '-introduction'\n",
    "    src = requests.get(url).text\n",
    "    soup = BeautifulSoup(src,'html.parser')\n",
    "    arr = np.array([])\n",
    "    \n",
    "    if year in [2016, 2017]:\n",
    "        for i in soup.find_all('tr'):\n",
    "            arr = np.concatenate((arr,[i.text.split()[2][1:-1]]))\n",
    "    elif year in [2018, 2019]:\n",
    "        top20 = soup.find_all('blockquote')[1].text.strip()\n",
    "        top20list = re.compile(\"[0-9]+\\.  \").split(top20)\n",
    "        for player in top20list:\n",
    "            if not len(player) <= 1:\n",
    "                arr = np.concatenate((arr,[player.split('\"')[1]]))\n",
    "    else:\n",
    "        print('Error')\n",
    "    return arr\n",
    "\n",
    "\n",
    "def aggregateTop20():  \n",
    "    top20indices = {2016: 19558, 2017: 22348, 2018: 25735, 2019: 28749}\n",
    "\n",
    "    top20 = np.array([])\n",
    "    for year in top20indices:\n",
    "        top20 = np.concatenate((top20,getTop20(year, top20indices[year]))) \n",
    "        time.sleep(1)\n",
    "    df = pd.DataFrame(top20.reshape(4,20).swapaxes(0,1), columns=list(top20indices.keys()))\n",
    "    df.index = np.arange(1,21)\n",
    "    return df\n",
    "\n",
    "top20DF = aggregateTop20()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_checkIfTop20(row,top20)_** is a function that is to be applied across a DataFrame of all the players. It takes in a DataFrame row as well as a Series that contains the Top 20 based on the provided year. An example is shown below with the players from 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIfTop20(row,top20):\n",
    "    name = row.name\n",
    "    if top20.str.match(name).sum() > 0:\n",
    "        return True\n",
    "    else:    \n",
    "        return False\n",
    "\n",
    "df = pd.read_csv('unfilteredplayers/players2019.csv',index_col=0)\n",
    "df.insert(df.shape[1],'HLTV Top 20',df.apply(checkIfTop20,axis=1,args=(top20DF.loc[:,2019],)))\n",
    "df[df['HLTV Top 20']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction is based on the three tabs of a player: Overview, Individual, and Matches. A total of 24 statistics will be used as features. Maps are complementary and necessary to obtain as they will vary the weighting on the ratings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_getRating(sp)_** is a helper function that takes a player's rating against top 5, 10, 20, 30, and 50 opponents. Additionally, the map counts against each of the 5 types of opponents are taken down so that they can be scaled. **_getFromOverview(url,arr)_** is the main function that fills in a stats array with the statistics that can be taken from the player's overview page. \n",
    "\n",
    "From **_getRating()_**:\n",
    "- Rating Avg (0): Rating 2.0 averaged across all maps played in year\n",
    "- Rating 5 (1): Rating 2.0 against top 5 opponents\n",
    "- Rating 10 (2): Rating 2.0 against top 10 opponents\n",
    "- Rating 20 (3): Rating 2.0 against top 20 opponents\n",
    "- Rating 30 (4): Rating 2.0 against top 30 opponents\n",
    "- Rating 50 (5): Rating 2.0 against top 50 opponents\n",
    "- Maps 5 (6): Maps against top 5 opponents\n",
    "- Maps 10 (7): Maps against top 10 opponents\n",
    "- Maps 20 (8): Maps against top 20 opponents\n",
    "- Maps 30 (9): Maps against top 30 opponents\n",
    "- Maps 50 (10): Maps against top 50 opponents\n",
    "\n",
    "The rest:\n",
    "- ADR (11): Average Damage per round\n",
    "- KPR (12): Kills per round\n",
    "- DPR (13): Deaths per round\n",
    "- APR (14): Assists per round\n",
    "- IMPACT (15): Quanitatively measures multikills, clutches, first bloods\n",
    "- KAST (16): % of rounds in which a player had a kill, assist, survived, or was traded\n",
    "- Grenade dmg (17): Grenade damage per round\n",
    "- HS% (18): % of kills that were headshots\n",
    "- Rounds played (19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRating(sp,a):\n",
    "    ratingsTab = sp.find('div',class_='featured-ratings-container')\n",
    "    scales = {'vs top 5 opponents': 1,'vs top 10 opponents':2,'vs top 20 opponents':3,\n",
    "                       'vs top 30 opponents':4,'vs top 50 opponents': 5}\n",
    "\n",
    "    for rating in ratingsTab.find_all('div',class_='rating-breakdown'):\n",
    "        ratingtype = rating.find('div',class_='rating-description').text\n",
    "        mapcount = int(rating.find('div',class_='rating-maps').text[1:-1].split()[0])\n",
    "        if mapcount == 0:\n",
    "            continue \n",
    "        \n",
    "        temp = float(rating.find('div',class_='rating-value').text)\n",
    "        a[scales[ratingtype]] = temp  \n",
    "        a[scales[ratingtype] + 5] = mapcount\n",
    "    return a\n",
    "\n",
    "\n",
    "def getFromOverview(url, arr):\n",
    "    src = requests.get(url).text\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "    statsIndex = {'Rating': 0, 'Rating5': 1,'Rating10': 2,'Rating20': 3, 'Rating30': 4, 'Rating50': 5,              \n",
    "                  'ADR': 11, 'KPR': 12, 'DPR': 13, 'Assists / round': 14, 'Impact': 15, 'KAST': 16, 'Grenade dmg / Round': 17,\n",
    "                  'Headshot %': 18, 'Rounds played': 19}\n",
    "    \n",
    "    # Ratin 2.0, ADR, KPR, DPR, KAST, IMPACT\n",
    "    for i in soup.find_all('div',class_=re.compile('summaryStatBreakdown ')):\n",
    "        statname = i.find('div',class_='summaryStatBreakdownSubHeader').text.split()[0]\n",
    "        if statname in statsIndex:\n",
    "            if statname == 'KAST':\n",
    "                arr[statsIndex[statname]] = i.find('div',class_='summaryStatBreakdownDataValue').text[:-1]\n",
    "            else:\n",
    "                arr[statsIndex[statname]] = i.find('div',class_='summaryStatBreakdownDataValue').text\n",
    "    \n",
    "    # APR, Grenade Dmg/ Round\n",
    "    for i in soup.find_all('div',class_='stats-row'):\n",
    "        if i.text.find('Grenade dmg / Round') > -1:\n",
    "            arr[statsIndex['Grenade dmg / Round']] = i.find_all('span')[1].text\n",
    "        elif i.text.find('Assists / round') > -1:\n",
    "            arr[statsIndex['Assists / round']] = i.find_all('span')[1].text\n",
    "        elif i.text.find('Headshot %') > -1:\n",
    "            arr[statsIndex['Headshot %']] = i.find_all('span')[1].text[:-1]\n",
    "        elif i.text.find('Rounds played') > -1:\n",
    "            arr[statsIndex['Rounds played']] = i.find_all('span')[1].text\n",
    "                \n",
    "        \n",
    "    # Rating Scale\n",
    "    arr = getRating(soup, arr)\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_getFromIndividual(url,arr)_** is the main function that fills in a stats array with the statistics that can be taken from the player's Individual page. \n",
    "\n",
    "- Rounds with kills (20)\n",
    "- k-d diff (21): Kills minus Deaths\n",
    "- Opening kill ratio (22)\n",
    "- Opening kill rating (23)\n",
    "- Team win percent after 1st kill (24)\n",
    "- First kill in won rounds (25)\n",
    "- Kill (26)\n",
    "- Death(27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFromIndividual(url, arr):\n",
    "    src = requests.get(url).text\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "    statsIndex = {'Rounds with kills': 20, 'Kill - Death difference': 21, 'Opening kill ratio': 22, 'Opening kill rating': 23,\n",
    "                  'Team win percent after first kill': 24, 'First kill in won rounds': 25, 'Kills': 26, 'Deaths': 27}\n",
    "    for i in soup.find_all('div',class_='stats-row'):\n",
    "        statname = i.span.text\n",
    "        if statname in statsIndex:\n",
    "            if statname == 'Kill - Death difference':\n",
    "                arr[statsIndex[statname]] = i.span.next_sibling.next_sibling.text\n",
    "            elif statname == 'Team win percent after first kill':\n",
    "                arr[statsIndex[statname]] = i.span.next_sibling.text[:-1]\n",
    "            elif statname == 'First kill in won rounds':\n",
    "                arr[statsIndex[statname]] = i.span.next_sibling.text[:-1]\n",
    "            else:\n",
    "                arr[statsIndex[statname]] = i.span.next_sibling.text\n",
    "                \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_getFromMatches(url,arr)_** is the main function that fills in a stats array with the statistics that can be taken from the player's Matches page. \n",
    "\n",
    "- % of maps with 1+ rating (28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFromMatches(url, arr):\n",
    "    src = requests.get(url).text\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "    statsIndex = {'Maps with 1+ rating': 28}\n",
    "    for i in soup.find_all('div',class_='col'):\n",
    "        statname = i.find('div',class_='description').text\n",
    "        if statname is None:\n",
    "            continue\n",
    "        elif statname in statsIndex:\n",
    "            arr[statsIndex[statname]] = i.find('div',class_='value').text[:-1]\n",
    "            break\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_getFeatures(row)_** is applied across an entire DataFrame and uses the three functions defined above. It takes in a row and returns a numpy array that contains the statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(row):\n",
    "    statsarr = np.zeros((29,))\n",
    "    \n",
    "    print(\"Working on:\" + str(row.name))\n",
    "    \n",
    "    overvUrl = row['Webpage']\n",
    "    statsarr = getFromOverview(overvUrl,statsarr)\n",
    "    time.sleep(7.5)\n",
    "    indivUrl = overvUrl[:31] + 'individual/' + overvUrl[31:]\n",
    "    statsarr = getFromIndividual(indivUrl,statsarr)\n",
    "    time.sleep(7.5)\n",
    "    matchUrl = overvUrl[:31] + 'matches/' + overvUrl[31:]\n",
    "    statsarr = getFromMatches(matchUrl,statsarr)\n",
    "    time.sleep(7.5)\n",
    "    return statsarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since applying **_getFeatures_** returns a Series of 12x1 numpy arrays, this nested structure needs to be flattened into a non-nested two-dimensional array. **_flattenFeatures(s,df)** extracts the nested structure made from the applied function and combines it with the existing DataFrame that came from the end of section 3-1. The Series is first converted into a numpy array, then numpy's **stack** function will flatten it. It is then concatenated with the original DataFrame that just had player names and hltv URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenFeatures(s,df):\n",
    "    flat = np.stack(s.to_numpy())\n",
    "   \n",
    "    featureNames = np.array(['Rating','RatingV5','RatingV10','RatingV20','RatingV30','RatingV50',\n",
    "                             'MapsV5','MapsV10','MapsV20','MapsV30','MapsV50',\n",
    "                             'ADR','KPR','DPR','APR', 'Impact','KAST','NadeDPR','HS%','Rounds played',\n",
    "                             'Rounds with kills', 'K-D Diff', 'Opening kill ratio', 'Opening kill rating',\n",
    "                             'Team win % after 1st kill', 'First kill in won rounds','Kills','Deaths',\n",
    "                             '% of maps with 1+ rating'])\n",
    "    features = pd.DataFrame(data=flat, columns=featureNames)\n",
    "    features.index = df.index\n",
    "    \n",
    "    final = pd.concat([df,features],axis=1)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since applying **_extractFinalFeature_** is the main function for feature extraction. It takes in a year as well as the top20DF created in section 2. It reads in the DataFrame saved in section 3-1 and then adds a column checking whether or not a player made it in that year's HLTV top 20. It will then call **_getFeatures_** to produce the Series of numpy arrays that contains all the stats. That series is inserted along with the DataFrame into **_flattenFeatures_** to produce a combined DataFrame with all the features necessary for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFinalFeatures(year):\n",
    "    top20DF = aggregateTop20()\n",
    "    df = pd.read_csv('unfilteredplayers/players' + str(year) + '.csv',index_col=0).iloc[:5,:]\n",
    "    df.insert(df.shape[1],'HLTV Top 20',df.apply(checkIfTop20,axis=1,args=(top20DF.loc[:,year],)))\n",
    "    \n",
    "    statsonly = df.apply(getFeatures,axis=1)\n",
    "    \n",
    "    finalTab = flattenFeatures(statsonly,df)\n",
    "    \n",
    "    finalTab.to_csv('features/meme'+str(year)+'.csv') \n",
    "    \n",
    "    return\n",
    "\n",
    "# for year in [2016,2017,2018,2019]:\n",
    "#     extractFinalFeatures(year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
